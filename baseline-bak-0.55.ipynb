{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas.Version:\t1.1.5\n",
      "numpy.Version:\t1.20.0\n",
      "tqdm.Version:\t4.26.0\n",
      "seaborn.Version:\t0.9.0\n",
      "matplotlib.Version:\t2.2.3\n",
      "sklearn.Version:\t0.19.2\n",
      "lightgbm.Version:\t2.1.1\n",
      "scorecardpy.Version:\t0.1.9.2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: lrhao\n",
    "@software: jupyter\n",
    "@file: baseline.ipynb\n",
    "@time: 2020-12-11\n",
    "@description：\n",
    "\"\"\"\n",
    "import pandas \n",
    "import numpy \n",
    "import tqdm \n",
    "import os\n",
    "import seaborn \n",
    "import math\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import time\n",
    "import lightgbm\n",
    "import random\n",
    "import scorecardpy\n",
    "\n",
    "print(\"pandas.Version:\\t\"+pd.__version__)\n",
    "print(\"numpy.Version:\\t\"+np.__version__)\n",
    "print(\"tqdm.Version:\\t\"+tqdm.__version__)\n",
    "print(\"seaborn.Version:\\t\"+seaborn.__version__)\n",
    "print(\"matplotlib.Version:\\t\"+matplotlib.__version__)\n",
    "print(\"sklearn.Version:\\t\"+sklearn.__version__)\n",
    "print(\"lightgbm.Version:\\t\"+lightgbm.__version__)\n",
    "print(\"scorecardpy.Version:\\t\"+scorecardpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import scorecardpy as sc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 21), (15000, 20), (15000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../../../train.csv')\n",
    "test = pd.read_csv('../../../test.csv')\n",
    "submit = pd.read_csv('../../../submit.csv')\n",
    "train.shape, test.shape, submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis = 0).reset_index(drop = True)\n",
    "#ZHIWU=1数据无意义，剔除该类数据\n",
    "df = df[df['ZHIWU']!=1]\n",
    "#剔除无效指标\n",
    "del df['ZHIWU']\n",
    "del df['XUELI']\n",
    "del df['HYZK']\n",
    "del df['ZHIYE']\n",
    "del df['ZHICHEN']\n",
    "del df['DWYJCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对数值类的进行转化：\n",
    "df['GRYJCE'] = df['GRYJCE'] - 237\n",
    "df['GRJCJS'] = df['GRJCJS'] -237 \n",
    "df['GRZHYE'] = df['GRZHYE'] - 237\n",
    "df['GRZHSNJZYE'] = df ['GRZHSNJZYE'] -237\n",
    "df['GRZHDNGJYE'] = df ['GRZHDNGJYE'] -237\n",
    "df['DKFFE'] = df['DKFFE'] - 237\n",
    "df['DKYE'] = df['DKYE']-237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate_2_cols = ['XINGBIE']\n",
    "cate_cols = [ 'DWJJLX', 'DWSSHY', 'GRZHZT']\n",
    "# 记录需要做embedding的变量\n",
    "cEmb = ['DWJJLX','DWSSHY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['GRJCJS', 'GRZHYE', 'GRZHSNJZYE', 'GRZHDNGJYE', 'GRYJCE','DKFFE', 'DKYE', 'DKLL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['missing_rate'] = (df.shape[1] - df.count(axis = 1)) / df.shape[1]\n",
    "\n",
    "df['DKFFE_DKYE'] = df['DKFFE'] + df['DKYE']\n",
    "df['DKFFE_DKY_multi_DKLL'] = (df['DKFFE'] + df['DKYE']) * df['DKLL']\n",
    "df['GRZHDNGJYE_GRZHSNJZYE'] = df['GRZHDNGJYE'] + df['GRZHSNJZYE']\n",
    "df['GRZHDNGJYE_GRZHSNJZYE1'] = df['GRZHDNGJYE'] - df['GRZHSNJZYE']\n",
    "df['GRZHDNGJYE_GRZHSNJZYE2'] = df['GRZHDNGJYE']*df['GRZHDNGJYE'] + df['GRZHSNJZYE'] * df['GRZHSNJZYE']\n",
    "\n",
    "df['DKFFE_multi_DKLL_ratio'] = df['DKFFE'] * df['DKLL'] / df['DKFFE_DKY_multi_DKLL']\n",
    "df['DKYE_multi_DKLL_ratio'] = df['DKYE'] * df['DKLL'] / df['DKFFE_DKY_multi_DKLL']\n",
    "\n",
    "df['GRZHYE_diff_GRZHDNGJYE'] = df['GRZHYE'] - df['GRZHDNGJYE']*12\n",
    "df['GRZHYE_diff_GRZHSNJZYE'] = df['GRZHYE'] - df['GRZHSNJZYE']*1.3\n",
    "df['GRZHYE_diff_GRZHDNGJYE1'] = df['GRZHYE'] - df['GRZHDNGJYE']\n",
    "df['GRZHYE_diff_GRZHSNJZYE1'] = df['GRZHYE'] - df['GRZHSNJZYE']\n",
    "#鬼知道，可能反应收入？\n",
    "df['YEARPURINCM'] = df['GRZHYE']-df['GRZHSNJZYE']-df['GRZHDNGJYE']\n",
    "#应还贷款月份，剩余需还贷款月份,已还贷款月份\n",
    "df['HDZYF'] = df['DKFFE']*(df['DKLL']/100+1)/(df['GRYJCE']*2)\n",
    "df['HDSYYF'] = df['DKYE']*(df['DKLL']/100+1)/(df['GRYJCE']*2)\n",
    "df['HDYF'] = (df['DKFFE']-df['DKYE'])/(df['GRYJCE']*2)\n",
    "\n",
    "#应还总额\n",
    "df['HDZYFZE'] = df['DKFFE']*(df['DKLL']/100+1)\n",
    "#待还总额\n",
    "df['HDSYYFZE'] = df['DKYE']*(df['DKLL']/100+1)\n",
    "\n",
    "#特征工程衍生的代码\n",
    "# 图形旋转指标\n",
    "df ['JS_YE'] = df['GRZHSNJZYE'] -20000* df['GRZHYE'].apply(lambda x: math.log(x+1))\n",
    "df ['HDSYYF_HDZYF'] =df['HDSYYF'] -  df['HDZYF']*0.8\n",
    "df ['GYE_GJC'] =df['GRZHYE'] -  (500-abs(df['GRYJCE']-1000))*100\n",
    "df ['USAGE_RATE']=df['DKYE']/df['DKFFE']\n",
    "df ['YE_USAGE'] = df['GRZHYE'] - df ['USAGE_RATE']*50000-50000\n",
    "df ['YJCE2GRZHDNGJYE'] = df['GRYJCE']/ df ['GRZHDNGJYE']\n",
    "df ['GRYJCE_RATE'] = df['GRYJCE']*df['GRYJCE']/2000/2000 + df ['USAGE_RATE']*df ['USAGE_RATE']\n",
    "df ['GJJRATE'] = (df['GRYJCE'])/df['GRJCJS']                                                        # OKOKOK\n",
    "df ['GJJRATE_GEN'] = df['GRJCJS'] - 60000*df['GJJRATE']                                             # OKOKOK\n",
    "df ['GJJRATE_GEN3'] = df['GRJCJS'] - 200000*df['GJJRATE']                                           # OKOKOK\n",
    "df ['GJJRATE_GEN2'] = df['GRYJCE'] - (600-df['GJJRATE']*2000)                                       # OKOKOK  \n",
    "df ['DKFFE_DKYE251']= df['DKFFE'] - df['DKFFE']/df['DKYE']*251\n",
    "df['GJJRATE_DIFF'] = df['GJJRATE'].apply(lambda x: 1 if x>0.1 else 0)\n",
    "\n",
    "# 金额取整取余指标   # 这些指标暂时没有放到后面的指标衍生过程中！\n",
    "df ['DKLT']= df['DKFFE']%100000\n",
    "df ['DKZS']= df['DKFFE']//100000  \n",
    "df ['YELT']= df['DKYE']%100000\n",
    "df ['YEZS']= df['DKYE']//100000\n",
    "df ['ZHLT']= df['GRZHYE']%10000\n",
    "df ['ZHZS']= df['GRZHYE']//10000\n",
    "df ['GJLT']= df['GRZHDNGJYE']%100000\n",
    "df ['GJZS']= df['GRZHDNGJYE']//100000\n",
    "df ['SNLT']= df['GRZHSNJZYE']%10000\n",
    "df ['SNZS']= df['GRZHSNJZYE']//10000\n",
    "df ['YJLT']= df['GRYJCE']%500\n",
    "df ['YJZS']= df['GRYJCE']//500\n",
    "df['SUBMONTH'] =  (df['GRZHYE']/df['GRYJCE']).apply(lambda x: math.log(x+1))                        # NEW NEW NEW\n",
    "df['YJCE_LOG'] = (df['GRYJCE']).apply(lambda x: math.log(x+1))                                      # NEW NEW NEW\n",
    "df['MONTH_LOG'] = df['SUBMONTH']-df['YJCE_LOG']                                                     # NEW NEW NEW\n",
    "\n",
    "df['USAGEOVER'] = df['DKYE']==0 \n",
    "\n",
    "gen_feats =  ['DKLT','YELT','ZHLT','GJLT','SNLT','YJLT','SUBMONTH','YJCE_LOG','MONTH_LOG',\n",
    "              'GYE_GJC','USAGE_RATE','YE_USAGE','YJCE2GRZHDNGJYE',\n",
    "           'GRYJCE_RATE','GJJRATE','GJJRATE_GEN','GJJRATE_GEN2','DKFFE_DKYE251',\n",
    "'GRZHYE_diff_GRZHSNJZYE', 'GRZHYE_diff_GRZHDNGJYE1','GRZHYE_diff_GRZHSNJZYE1', 'YEARPURINCM','HDZYF',\n",
    "            'HDSYYF','HDYF','HDZYFZE','HDSYYFZE','JS_YE','HDSYYF_HDZYF',\n",
    "'DKFFE_DKYE', 'DKFFE_DKY_multi_DKLL','GRZHDNGJYE_GRZHSNJZYE','GRZHDNGJYE_GRZHSNJZYE1',  'GRZHDNGJYE_GRZHSNJZYE2',\n",
    "             'DKFFE_multi_DKLL_ratio', 'DKYE_multi_DKLL_ratio', 'GRZHYE_diff_GRZHDNGJYE']\n",
    "            \n",
    "cate_cols += ['DKZS','YEZS','ZHZS','GJZS','SNZS','YJZS','USAGEOVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[898, 919, 778, 845]\n"
     ]
    }
   ],
   "source": [
    "#read in the feature importance data\n",
    "validIndics = list()\n",
    "validIndicsAll = list()\n",
    "level = 30\n",
    "for i in range(1,5):\n",
    "    dimp = pd.read_csv(f\"FImp{i}.csv\")\n",
    "    validIndics.append(dimp[dimp['importance']>level]['column'].values.tolist())\n",
    "    validIndicsAll +=  dimp[dimp['importance']>level]['column'].values.tolist()\n",
    "print([len(i) for i in validIndics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def get_age(df,col = 'age'):\n",
    "    df[col+\"_genFeat1\"]=(df['age'] > 18).astype(int)\n",
    "    df[col+\"_genFeat2\"]=(df['age'] > 25).astype(int)\n",
    "    df[col+\"_genFeat3\"]=(df['age'] > 30).astype(int)\n",
    "    df[col+\"_genFeat4\"]=(df['age'] > 35).astype(int)\n",
    "    df[col+\"_genFeat5\"]=(df['age'] > 40).astype(int)\n",
    "    df[col+\"_genFeat6\"]=(df['age'] > 45).astype(int)\n",
    "    return df, [col + f'_genFeat{i}' for i in range(1, 7)]\n",
    "\n",
    "# BORN MONTH\n",
    "tNow = time.mktime(time.strptime( '2021-01-12 00:00:00','%Y-%m-%d %H:%M:%S'))\n",
    "df['bornmonth'] = df['CSNY'].apply(lambda x: time.localtime(x)[1])\n",
    "\n",
    "#这个怎么换算出来的啊，超哥\n",
    "df['age'] = ((1609430399 - df['CSNY']) / (365 * 24 * 3600)).astype(int)\n",
    "df, genFeats1 = get_age(df, col = 'age')\n",
    "\n",
    "#sns.distplot(df['age'][df['age'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_daikuanYE(df,col):\n",
    "    df[col + '_genFeat1'] = (df[col] > 100000).astype(int)\n",
    "    df[col + '_genFeat2'] = (df[col] > 120000).astype(int)\n",
    "    df[col + '_genFeat3'] = (df[col] > 140000).astype(int)\n",
    "    df[col + '_genFeat4'] = (df[col] > 180000).astype(int)\n",
    "    df[col + '_genFeat5'] = (df[col] > 220000).astype(int)\n",
    "    df[col + '_genFeat6'] = (df[col] > 260000).astype(int)\n",
    "    df[col + '_genFeat7'] = (df[col] > 300000).astype(int)\n",
    "    return df, [col + f'_genFeat{i}' for i in range(1, 8)]\n",
    "\n",
    "df, genFeats2 = get_daikuanYE(df, col = 'DKYE')\n",
    "df, genFeats3 = get_daikuanYE(df, col = 'DKFFE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.84it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 119.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.54it/s]\n",
      "100%|██████████| 45/45 [00:08<00:00,  5.15it/s]\n",
      "100%|██████████| 45/45 [00:27<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# One-hot 编码\n",
    "for f in tqdm(cate_cols):\n",
    "    df[f] = df[f].map(dict(zip(df[f].unique(), range(df[f].nunique()))))\n",
    "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    df = pd.concat([df,pd.get_dummies(df[f],prefix=f\"{f}\")],axis=1)\n",
    "\n",
    "#离散型特征关联\n",
    "cate_cols_combine = [[cate_cols[i], cate_cols[j]] for i in range(len(cate_cols)) \\\n",
    "                     for j in range(i + 1, len(cate_cols))]\n",
    "\n",
    "#离散型联合计数占比\n",
    "for f1, f2 in tqdm(cate_cols_combine):\n",
    "    df['{}_{}_count'.format(f1, f2)] = df.groupby([f1, f2])['id'].transform('count')\n",
    "    df['{}_in_{}_prop'.format(f1, f2)] = df['{}_{}_count'.format(f1, f2)] / df[f2 + '_count']\n",
    "    df['{}_in_{}_prop'.format(f2, f1)] = df['{}_{}_count'.format(f1, f2)] / df[f1 + '_count']\n",
    "\n",
    "\n",
    "#离散型变量下各个数值型指标统计\n",
    "for f1 in tqdm(cate_cols):\n",
    "    g = df.groupby(f1)\n",
    "    for f2 in num_cols + gen_feats:\n",
    "        for stat in ['sum', 'mean', 'std', 'max', 'min', 'std']:\n",
    "            if '{}_{}_{}'.format(f1, f2, stat)  in validIndicsAll:\n",
    "                df['{}_{}_{}'.format(f1, f2, stat)] = g[f2].transform(stat)\n",
    "    for f3 in genFeats2 + genFeats3:\n",
    "        for stat in ['sum', 'mean']:\n",
    "            if '{}_{}_{}'.format(f1, f3, stat) in validIndicsAll:\n",
    "                df['{}_{}_{}'.format(f1, f3, stat)] = g[f3].transform(stat)\n",
    "\n",
    "    \n",
    "#离散型变量下各个数值特征指标\n",
    "num_cols_gen_feats = num_cols + gen_feats\n",
    "for f1 in tqdm(num_cols_gen_feats):\n",
    "    g = df.groupby(f1)\n",
    "    for f2 in num_cols_gen_feats:\n",
    "        if f1 != f2:\n",
    "            for stat in ['sum', 'mean', 'std', 'max', 'min', 'std']:\n",
    "                if '{}_{}_{}'.format(f1, f2, stat) in validIndicsAll:\n",
    "                    df['{}_{}_{}'.format(f1, f2, stat)] = g[f2].transform(stat)\n",
    "\n",
    "#连续性数值间的特征工程\n",
    "for i in tqdm(range(len(num_cols_gen_feats))):\n",
    "    for j in range(i + 1, len(num_cols_gen_feats)):\n",
    "        if f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_add'  in validIndicsAll:\n",
    "            df[f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_add'] = df[num_cols_gen_feats[i]] + df[num_cols_gen_feats[j]]\n",
    "        if f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_diff'  in validIndicsAll:\n",
    "            df[f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_diff'] = df[num_cols_gen_feats[i]] - df[num_cols_gen_feats[j]]\n",
    "        if f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_multi'  in validIndicsAll:\n",
    "            df[f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_multi'] = df[num_cols_gen_feats[i]] * df[num_cols_gen_feats[j]]\n",
    "        if f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_div'  in validIndicsAll:\n",
    "            df[f'numsOf_{num_cols_gen_feats[i]}_{num_cols_gen_feats[j]}_div'] = df[num_cols_gen_feats[i]] / (df[num_cols_gen_feats[j]] + 0.0000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39994, 2919)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2919)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = df[df['label'].isna() == False].reset_index(drop=True)\n",
    "test_df = df[df['label'].isna() == True].reset_index(drop=True)\n",
    "display(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, ['missing_rate', 'ZHZS_31', 'SNZS_30', 'SNZS_31'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_feats = []\n",
    "drop_feats = [f for f in train_df.columns if train_df[f].nunique() == 1 or train_df[f].nunique() == 0]\n",
    "len(drop_feats), drop_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    \n",
    "    return ('TPR',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "import time\n",
    "from lightgbm.callback import reset_parameter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898 1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['DWJJLX', 'DWSSHY']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0427553\n",
      "[400]\tvalid_0's binary_error: 0.0336292\n",
      "[600]\tvalid_0's binary_error: 0.0325041\n",
      "Early stopping, best iteration is:\n",
      "[567]\tvalid_0's binary_error: 0.032379\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0405051\n",
      "[400]\tvalid_0's binary_error: 0.0336292\n",
      "[600]\tvalid_0's binary_error: 0.031629\n",
      "[800]\tvalid_0's binary_error: 0.031629\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_error: 0.0312539\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0452557\n",
      "[400]\tvalid_0's binary_error: 0.0382548\n",
      "[600]\tvalid_0's binary_error: 0.0356295\n",
      "[800]\tvalid_0's binary_error: 0.0342543\n",
      "[1000]\tvalid_0's binary_error: 0.0332542\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_error: 0.0330041\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0471309\n",
      "[400]\tvalid_0's binary_error: 0.0382548\n",
      "[600]\tvalid_0's binary_error: 0.0365046\n",
      "[800]\tvalid_0's binary_error: 0.0358795\n",
      "Early stopping, best iteration is:\n",
      "[708]\tvalid_0's binary_error: 0.0356295\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0425106\n",
      "[400]\tvalid_0's binary_error: 0.0352588\n",
      "[600]\tvalid_0's binary_error: 0.0333833\n",
      "[800]\tvalid_0's binary_error: 0.0325081\n",
      "[1000]\tvalid_0's binary_error: 0.032008\n",
      "[1200]\tvalid_0's binary_error: 0.0316329\n",
      "Early stopping, best iteration is:\n",
      "[1034]\tvalid_0's binary_error: 0.0315079\n",
      "======== model0 TPR:0.562242\n",
      "919 1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['DWJJLX', 'DWSSHY']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0415052\n",
      "[400]\tvalid_0's binary_error: 0.0338792\n",
      "[600]\tvalid_0's binary_error: 0.032379\n",
      "[800]\tvalid_0's binary_error: 0.031754\n",
      "[1000]\tvalid_0's binary_error: 0.0313789\n",
      "Early stopping, best iteration is:\n",
      "[902]\tvalid_0's binary_error: 0.0312539\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0386298\n",
      "[400]\tvalid_0's binary_error: 0.031879\n",
      "[600]\tvalid_0's binary_error: 0.0300038\n",
      "Early stopping, best iteration is:\n",
      "[518]\tvalid_0's binary_error: 0.0300038\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0452557\n",
      "[400]\tvalid_0's binary_error: 0.0358795\n",
      "[600]\tvalid_0's binary_error: 0.0331291\n",
      "[800]\tvalid_0's binary_error: 0.0325041\n",
      "[1000]\tvalid_0's binary_error: 0.0325041\n",
      "Early stopping, best iteration is:\n",
      "[844]\tvalid_0's binary_error: 0.032004\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0475059\n",
      "[400]\tvalid_0's binary_error: 0.0382548\n",
      "[600]\tvalid_0's binary_error: 0.0361295\n",
      "[800]\tvalid_0's binary_error: 0.0355044\n",
      "[1000]\tvalid_0's binary_error: 0.0351294\n",
      "Early stopping, best iteration is:\n",
      "[858]\tvalid_0's binary_error: 0.0350044\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.040135\n",
      "[400]\tvalid_0's binary_error: 0.0347587\n",
      "[600]\tvalid_0's binary_error: 0.0332583\n",
      "[800]\tvalid_0's binary_error: 0.0327582\n",
      "[1000]\tvalid_0's binary_error: 0.0322581\n",
      "Early stopping, best iteration is:\n",
      "[976]\tvalid_0's binary_error: 0.032133\n",
      "======== model1 TPR:0.561444\n",
      "778 1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['DWJJLX', 'DWSSHY']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0415052\n",
      "[400]\tvalid_0's binary_error: 0.0332542\n",
      "[600]\tvalid_0's binary_error: 0.0315039\n",
      "[800]\tvalid_0's binary_error: 0.0310039\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's binary_error: 0.0308789\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.03988\n",
      "[400]\tvalid_0's binary_error: 0.0341293\n",
      "[600]\tvalid_0's binary_error: 0.0313789\n",
      "[800]\tvalid_0's binary_error: 0.0301288\n",
      "[1000]\tvalid_0's binary_error: 0.0296287\n",
      "[1200]\tvalid_0's binary_error: 0.0295037\n",
      "Early stopping, best iteration is:\n",
      "[1015]\tvalid_0's binary_error: 0.0292537\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0442555\n",
      "[400]\tvalid_0's binary_error: 0.0348794\n",
      "[600]\tvalid_0's binary_error: 0.0331291\n",
      "[800]\tvalid_0's binary_error: 0.0330041\n",
      "Early stopping, best iteration is:\n",
      "[669]\tvalid_0's binary_error: 0.032379\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0455057\n",
      "[400]\tvalid_0's binary_error: 0.0372547\n",
      "[600]\tvalid_0's binary_error: 0.0358795\n",
      "[800]\tvalid_0's binary_error: 0.0356295\n",
      "[1000]\tvalid_0's binary_error: 0.0350044\n",
      "Early stopping, best iteration is:\n",
      "[860]\tvalid_0's binary_error: 0.0347543\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.040135\n",
      "[400]\tvalid_0's binary_error: 0.0331333\n",
      "Early stopping, best iteration is:\n",
      "[363]\tvalid_0's binary_error: 0.0326332\n",
      "======== model2 TPR:0.568371\n",
      "845 1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['DWJJLX', 'DWSSHY']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0422553\n",
      "[400]\tvalid_0's binary_error: 0.0341293\n",
      "[600]\tvalid_0's binary_error: 0.0332542\n",
      "[800]\tvalid_0's binary_error: 0.0325041\n",
      "Early stopping, best iteration is:\n",
      "[761]\tvalid_0's binary_error: 0.032129\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0390049\n",
      "[400]\tvalid_0's binary_error: 0.0328791\n",
      "[600]\tvalid_0's binary_error: 0.0313789\n",
      "Early stopping, best iteration is:\n",
      "[578]\tvalid_0's binary_error: 0.0311289\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0455057\n",
      "[400]\tvalid_0's binary_error: 0.0357545\n",
      "[600]\tvalid_0's binary_error: 0.0336292\n",
      "[800]\tvalid_0's binary_error: 0.032004\n",
      "[1000]\tvalid_0's binary_error: 0.031629\n",
      "[1200]\tvalid_0's binary_error: 0.031879\n",
      "Early stopping, best iteration is:\n",
      "[1038]\tvalid_0's binary_error: 0.0313789\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0440055\n",
      "[400]\tvalid_0's binary_error: 0.0375047\n",
      "[600]\tvalid_0's binary_error: 0.0363795\n",
      "[800]\tvalid_0's binary_error: 0.0353794\n",
      "[1000]\tvalid_0's binary_error: 0.0351294\n",
      "Early stopping, best iteration is:\n",
      "[914]\tvalid_0's binary_error: 0.0347543\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's binary_error: 0.0422606\n",
      "[400]\tvalid_0's binary_error: 0.0347587\n",
      "[600]\tvalid_0's binary_error: 0.0336334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f839011df0fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                        \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     )\n\u001b[1;32m     34\u001b[0m             \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    677\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    471\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = np.zeros((len(validIndics),train_df.shape[0]))\n",
    "middleresult =  np.zeros((len(validIndics),test_df.shape[0]))\n",
    "seeds = [1023]\n",
    "for imodel in range(len(validIndics)):\n",
    "\n",
    "    clf = LGBMClassifier(\n",
    "    boosting_type = 'goss',\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=4023,\n",
    "    num_leaves=101,  # 只进行两层叶子节点的交叉\n",
    "    random_state=1023,\n",
    "    metric=None,\n",
    "    is_unbalance=True,\n",
    "    reg_alpha=0.005,\n",
    "    reg_lambda=0.005,\n",
    "    min_data_in_leaf = 5,\n",
    "    )\n",
    "    cols = list(set([col for col in train_df.columns if col not in ['id', 'label'] + drop_feats]).intersection(set(validIndics[imodel])))\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(len(cols),seed)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for i, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "            trn_x, trn_y = train_df[cols].iloc[trn_idx].reset_index(drop=True), train_df['label'].values[trn_idx]\n",
    "            val_x, val_y = train_df[cols].iloc[val_idx].reset_index(drop=True), train_df['label'].values[val_idx]\n",
    "            clf.fit(\n",
    "                        trn_x, trn_y,\n",
    "                        eval_set=[(val_x, val_y)],\n",
    "                        categorical_feature=list(set(cate_cols).intersection(cols)),\n",
    "                        eval_metric='binary_error',\n",
    "                       early_stopping_rounds=200,\n",
    "                       verbose=200\n",
    "                    )\n",
    "            oof[imodel,val_idx] = clf.predict_proba(val_x)[:, 1]\n",
    "            middleresult[imodel] += clf.predict_proba(test_df[cols])[:, 1] / skf.n_splits / len(seeds)     \n",
    "        tpr = round(tpr_weight_funtion(train_df['label'],oof[imodel])[1], 6)\n",
    "        print(\"======== model\"+str(imodel)+' TPR:'+str(tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stackDF_train = pd.DataFrame(oof.T)\n",
    "stackDF_train['label'] = train_df['label']\n",
    "stackDF_test = pd.DataFrame(middleresult.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.562423\n",
      "0.556075\n",
      "0.561589\n",
      "0.556692\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in stackDF_train.columns:\n",
    "    print(round(tpr_weight_funtion(train_df['label'],stackDF_train[i])[1], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy('TPR', 0.5170648464163823, True)\n",
      "adjusted_mutual_info_score('TPR', 0.5170648464163823, True)\n",
      "adjusted_rand_score('TPR', 0.5170648464163823, True)\n",
      "average_precision('TPR', 0.5102389078498294, True)\n",
      "balanced_accuracy('TPR', 0.5102389078498294, True)\n",
      "completeness_score('TPR', 0.5170648464163823, True)\n",
      "explained_variance('TPR', 0.5170648464163823, True)\n",
      "f1('TPR', 0.5170648464163823, True)\n",
      "f1_macro('TPR', 0.5170648464163823, True)\n",
      "f1_micro('TPR', 0.5170648464163823, True)\n",
      "f1_weighted('TPR', 0.5170648464163823, True)\n",
      "fowlkes_mallows_score('TPR', 0.5170648464163823, True)\n",
      "homogeneity_score('TPR', 0.5136518771331058, True)\n",
      "jaccard('TPR', 0.5170648464163823, True)\n",
      "jaccard_macro('TPR', 0.5170648464163823, True)\n",
      "jaccard_micro('TPR', 0.5170648464163823, True)\n",
      "jaccard_weighted('TPR', 0.5170648464163823, True)\n",
      "max_error('TPR', 0.5170648464163823, True)\n",
      "mutual_info_score('TPR', 0.5136518771331058, True)\n",
      "neg_brier_score('TPR', 0.5102389078498294, True)\n",
      "neg_log_loss('TPR', 0.5180887372013652, True)\n",
      "neg_mean_absolute_error('TPR', 0.5170648464163823, True)\n",
      "neg_mean_squared_error('TPR', 0.5170648464163823, True)\n",
      "neg_median_absolute_error('TPR', 0.5170648464163823, True)\n",
      "neg_root_mean_squared_error('TPR', 0.5170648464163823, True)\n",
      "normalized_mutual_info_score('TPR', 0.5170648464163823, True)\n",
      "precision('TPR', 0.5170648464163823, True)\n",
      "precision_macro('TPR', 0.5170648464163823, True)\n",
      "precision_micro('TPR', 0.5170648464163823, True)\n",
      "precision_weighted('TPR', 0.5170648464163823, True)\n",
      "r2('TPR', 0.5170648464163823, True)\n",
      "recall('TPR', 0.5102389078498294, True)\n",
      "recall_macro('TPR', 0.5102389078498294, True)\n",
      "recall_micro('TPR', 0.5170648464163823, True)\n",
      "recall_weighted('TPR', 0.5170648464163823, True)\n",
      "roc_auc('TPR', 0.5102389078498294, True)\n",
      "roc_auc_ovo('TPR', 0.5102389078498294, True)\n",
      "roc_auc_ovo_weighted('TPR', 0.5102389078498294, True)\n",
      "roc_auc_ovr('TPR', 0.5102389078498294, True)\n",
      "roc_auc_ovr_weighted('TPR', 0.5102389078498294, True)\n",
      "v_measure_score('TPR', 0.5170648464163823, True)\n"
     ]
    }
   ],
   "source": [
    "cols = list(set(stackDF_train.columns)-set(['label']))\n",
    "from sklearn.linear_model import LogisticRegressionCV,LinearRegression\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in sorted(sklearn.metrics.SCORERS.keys()):\n",
    "    try:\n",
    "        X_train,X_test,Y_train,Y_test = train_test_split(stackDF_train[cols],stackDF_train['label'],test_size=0.1,random_state=0)\n",
    "        lr = LogisticRegressionCV(fit_intercept=True,Cs=np.logspace(-2,2,20),cv=2,penalty=\"l2\",solver=\"lbfgs\",tol=0.01,\n",
    "                                  class_weight={0:0.1,1:1},scoring=i)\n",
    "        re = lr.fit(X_train,Y_train)\n",
    "\n",
    "        r = re.score(X_train,Y_train)\n",
    "        P_test = [i[1] for i in re.predict_proba(X_test)]\n",
    "        print(i+str(tpr_weight_funtion(Y_test,P_test)))\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegressionCV in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegressionCV(LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      " |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  See glossary entry for :term:`cross-validation estimator`.\n",
      " |  \n",
      " |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      " |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      " |  regularization with primal formulation. The liblinear solver supports both\n",
      " |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      " |  Elastic-Net penalty is only supported by the saga solver.\n",
      " |  \n",
      " |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      " |  is selected by the cross-validator\n",
      " |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      " |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  Cs : int or list of floats, default=10\n",
      " |      Each of the values in Cs describes the inverse of regularization\n",
      " |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      " |      in a logarithmic scale between 1e-4 and 1e4.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  cv : int or cross-validation generator, default=None\n",
      " |      The default cross-validation generator used is Stratified K-Folds.\n",
      " |      If an integer is provided, then it is the number of folds used.\n",
      " |      See the module :mod:`sklearn.model_selection` module for the\n",
      " |      list of possible cross-validation objects.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver.\n",
      " |  \n",
      " |  scoring : str or callable, default=None\n",
      " |      A string (see model evaluation documentation) or\n",
      " |      a scorer callable object / function with signature\n",
      " |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      " |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      " |      default scoring option used is 'accuracy'.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      " |        'liblinear' and 'saga' handle L1 penalty.\n",
      " |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      " |        not handle warm-starting.\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can preprocess the data\n",
      " |      with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations of the optimization algorithm.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         class_weight == 'balanced'\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used during the cross-validation loop.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      " |      positive number for verbosity.\n",
      " |  \n",
      " |  refit : bool, default=True\n",
      " |      If set to True, the scores are averaged across all folds, and the\n",
      " |      coefs and the C that corresponds to the best score is taken, and a\n",
      " |      final refit is done using these parameters.\n",
      " |      Otherwise the coefs, intercepts and C that correspond to the\n",
      " |      best scores across folds are averaged.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      " |      Note that this only applies to the solver and not the cross-validation\n",
      " |      generator. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  l1_ratios : list of float, default=None\n",
      " |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      " |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      " |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      " |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      " |      of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem\n",
      " |      is binary.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape(1,) when the problem is binary.\n",
      " |  \n",
      " |  Cs_ : ndarray of shape (n_cs)\n",
      " |      Array of C i.e. inverse of regularization parameter values used\n",
      " |      for cross-validation.\n",
      " |  \n",
      " |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      " |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      " |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      " |  \n",
      " |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      " |      dict with classes as the keys, and the path of coefficients obtained\n",
      " |      during cross-validating across each fold and then across each Cs\n",
      " |      after doing an OvR for the corresponding class as values.\n",
      " |      If the 'multi_class' option is set to 'multinomial', then\n",
      " |      the coefs_paths are the coefficients corresponding to each class.\n",
      " |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      " |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      " |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      " |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      " |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      " |  \n",
      " |  scores_ : dict\n",
      " |      dict with classes as the keys, and the values as the\n",
      " |      grid of scores obtained during cross-validating each fold, after doing\n",
      " |      an OvR for the corresponding class. If the 'multi_class' option\n",
      " |      given is 'multinomial' then the same scores are repeated across\n",
      " |      all classes, since this is the multinomial class. Each dict value\n",
      " |      has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      " |      ``penalty='elasticnet'``.\n",
      " |  \n",
      " |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      " |      Array of C that maps to the best scores across every class. If refit is\n",
      " |      set to False, then for each class, the best C is the average of the\n",
      " |      C's that correspond to the best scores for each fold.\n",
      " |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      " |  \n",
      " |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      " |      Array of l1_ratio that maps to the best scores across every class. If\n",
      " |      refit is set to False, then for each class, the best l1_ratio is the\n",
      " |      average of the l1_ratio's that correspond to the best scores for each\n",
      " |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      " |      Actual number of iterations for all classes, folds and Cs.\n",
      " |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      " |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      " |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :]).shape\n",
      " |  (2, 3)\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.98...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  LogisticRegression\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegressionCV\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the score using the `scoring` option on the given\n",
      " |      test data and labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Score of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LogisticRegression:\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegressionCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    stackDF_train[cols],stackDF_train['label'],\n",
    "            eval_metric='auc',\n",
    "           verbose=200,\n",
    "        )\n",
    "test_df['prob'] = clf.predict_proba(stackDF_test[cols])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#指标重要性分析\n",
    "dImp = pd.DataFrame({\n",
    "        'column': cols,\n",
    "        'importance': [sum(i)/len(i) for i in np.array([ i.feature_importances_ for i in modellist ]).transpose()] ,\n",
    "    }).sort_values(by='importance',ascending=False).reset_index()\n",
    "print(len(dImp[dImp['importance']<1]['column'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dImp.to_csv('FImp4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15000, 2)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['id'] = test_df['id']\n",
    "submit['label'] = test_df['prob']\n",
    "submit = pd.merge(test,submit,on='id',how='left')[['id','label']]\n",
    "submit['label'] = submit['label'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "\n",
    "\n",
    "submit.to_csv('../sub/submission{}_{}.csv'.format(tpr, round(np.mean(val_aucs), 6)), index = False)\n",
    "submit.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
